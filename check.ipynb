{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ragaai-catalyst -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check base_url and keys error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst import RagaAICatalyst\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=\"9Dr41iRXo4RhGLlLjnmq\",\n",
    "    secret_key=\"a7vEHZJrSdsZzkinTnXKZ3XNllzH0vu2EdooTNf5\",\n",
    "    base_url=\"https://llm-dev5.ragaai.ai/api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test_project_1', 'csv_data_upload1', 'test-rag-application', 'Test-RAG-v4', 'Test_project_ritika', 'Test_project_ritika3', 'Test-RAG-App-v3', 'csv1', 'Test-RAG-App-v2', 'test-rupali', 'workday', 'error_handling_test1', 'Test_project_ritika1', 'Test_project_6', 'test-project-8-9-24-v1', 'test-project-8-9-24', 'test-project-1', 'Test_project_4', 'Guardrails', 'Test_project_3', 'Test_project_2', 'Test-RAG-App-1', 'qwerty', 'docker_auto_test', 'goldcasts', 'Test_29_08 hh', 'diwakar', 'asflnafsalkfnasdsadasdsadadsadadasd', 'dasdsadasdsasdadasd', 'dasdsdasddadasadasdas', 'dada', 'sdadas', 'dadasd', 'dadasdasd', 'Test_Project_ab', 'nine-46', 'five-12', 'trace-test', 'Test3', 'redbus_false_refusal', 'redbus', 'halueval_test', 'nnnww', 'StressTesting', 'test-url', 'test_url', 'Test5', 'Test4', 'SaaSMetrics', 'test_diw_123', 'test_123', 'testing123', 'vijay_sql', 'honeywell_eval_gt', 'honeywell-eval', 'llm_judge_metrics_v5', 'sql-metric-uat-2', 'sql-metric-uat', 'try', 'Test_SJ', 'SQLTEST2', 'SQLTEST1', 'qang12', 'uattt', 'new-proj', 'new-13aug-1', 'new-13aug', 'newproject', 'fg', 'new_project', 'new-test', 'Ng1f', 'Ng', 'for-trace-view', 'new-feat', 'new-feature-1', 'gfjk', 'testdataset8aug', 'new', 'RagaAI_catlyst_7', 'newRS', 'qang81', 'qang18', 'testRS07', 'qang181', 'Project_73476345', 'project_1721651101', 'project_1721650196', 'project_1721648967', 'project_1721649068', 'project_1721648951', 'project_1721647444', 'project_1721647347', 'project_1721647175', 'project_1721647146', 'project_1721647138', 'project_1721646928', 'project_1721646847', 'project_1721646801', 'project_1721646635']\n"
     ]
    }
   ],
   "source": [
    "# Create a project\n",
    "# project = catalyst.create_project(\n",
    "#     project_name=\"test-project-8-9-24-v1\",\n",
    "#     description=\"Description of the project\"\n",
    "# )\n",
    "\n",
    "# List projects\n",
    "projects = catalyst.list_projects()\n",
    "print(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check project name error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracer started for project: Test_project_1\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst import Tracer\n",
    "# Start a trace recording\n",
    "tracer = Tracer(\n",
    "    project_name=\"Test_project_1\",\n",
    "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
    "    tracer_type=\"langchain\",\n",
    "    pipeline={\n",
    "        \"llm_model\": \"gpt-3.5-turbo\",\n",
    "        \"vector_store\": \"faiss\",\n",
    "        \"embed_model\": \"text-embedding-ada-002\",\n",
    "    }\n",
    ").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check tracer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Trace cannot be logged to this Project because of schema difference. Create a new project to log trace",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragaai_catalyst\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tracer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Start a trace recording\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tracer \u001b[38;5;241m=\u001b[39m \u001b[43mTracer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocker_auto_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlangchain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector_store\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfaiss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membed_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/ragaai-catalyst/ragaai_catalyst/tracers/tracer.py:83\u001b[0m, in \u001b[0;36mTracer.__init__\u001b[0;34m(self, project_name, tracer_type, pipeline, metadata, description, upload_timeout)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m project_list:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject not found. Please enter a valid project name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraga_client \u001b[38;5;241m=\u001b[39m \u001b[43mRagaExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracer_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_provider()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instrumentor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_instrumentor(tracer_type)\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/ragaai-catalyst/ragaai_catalyst/tracers/exporters/raga_exporter.py:58\u001b[0m, in \u001b[0;36mRagaExporter.__init__\u001b[0;34m(self, project_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAGAAI_CATALYST_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     57\u001b[0m     get_token()\n\u001b[0;32m---> 58\u001b[0m status_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m     60\u001b[0m     create_status_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_schema()\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/ragaai-catalyst/ragaai_catalyst/tracers/exporters/raga_exporter.py:124\u001b[0m, in \u001b[0;36mRagaExporter._check_schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m     is_same, _ \u001b[38;5;241m=\u001b[39m compare_schemas(base_schema, project_schema)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_same:\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrace cannot be logged to this Project because of schema difference. Create a new project to log trace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code\n",
      "\u001b[0;31mException\u001b[0m: Trace cannot be logged to this Project because of schema difference. Create a new project to log trace"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst import Tracer\n",
    "# Start a trace recording\n",
    "tracer = Tracer(\n",
    "    project_name=\"docker_auto_test\",\n",
    "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
    "    tracer_type=\"langchain\",\n",
    "    pipeline={\n",
    "        \"llm_model\": \"gpt-3.5-turbo\",\n",
    "        \"vector_store\": \"faiss\",\n",
    "        \"embed_model\": \"text-embedding-ada-002\",\n",
    "    }\n",
    ").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check experiment name error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragaai_catalyst import Experiment\n",
    "\n",
    "# Create an experiment\n",
    "experiment_manager = Experiment(\n",
    "    project_name=\"Test_project_1\",\n",
    "    experiment_name=\"Exp-01\",\n",
    "    experiment_description=\"Experiment Description\",\n",
    "    dataset_name=\"Dataset Created from UI\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_schemas(base_schema, new_schema):\n",
    "    differences = []\n",
    "    for key, base_value in base_schema.items():\n",
    "        if key not in new_schema:\n",
    "            differences.append(f\"Key '{key}' is missing in new schema.\")\n",
    "        else:\n",
    "            # Remove everything after '_' in the new schema value\n",
    "            new_value = new_schema[key].split('_')[0]\n",
    "            if base_value != new_value:\n",
    "                differences.append(f\"Value mismatch for key '{key}': base = '{base_value}', new = '{new_value}'.\")\n",
    "\n",
    "    if differences:\n",
    "        return False, differences\n",
    "    return True, []\n",
    "\n",
    "# Example usage:\n",
    "SCHEMA_MAPPING = {\n",
    "    \"trace_id\": \"traceId\",\n",
    "    \"trace_uri\": \"traceUri\",\n",
    "    \"prompt\": \"prompt\",\n",
    "    \"response\": \"response\",\n",
    "    \"context\": \"context\",\n",
    "    \"llm_model\": \"pipeline\",\n",
    "    \"recorded_on\": \"metadata\",\n",
    "    \"embed_model\": \"pipeline\",\n",
    "    \"log_source\": \"metadata\",\n",
    "    \"vector_store\": \"pipeline\",\n",
    "}\n",
    "\n",
    "NEW_SCHEMA = {\n",
    "    \"trace_id\": \"traceId\",\n",
    "    \"trace_uri\": \"traceUri\",\n",
    "    \"llm_model\": \"pipeline_1\",\n",
    "    \"embed_model\": \"pipeline_2\",\n",
    "    \"vector_store\": \"pipeline_3\",\n",
    "    \"feedback\": \"feedBack\",\n",
    "    \"prompt_length\": \"promptLength\",\n",
    "    \"response\": \"response\",\n",
    "    \"log_source\": \"metadata_2\",\n",
    "    \"context\": \"context\",\n",
    "    \"prompt_id\": \"promptId\",\n",
    "    \"prompt\": \"prompt\",\n",
    "    \"recorded_on\": \"metadata_1\"\n",
    "}\n",
    "\n",
    "# Check schemas\n",
    "# result, diff = compare_schemas(SCHEMA_MAPPING, NEW_SCHEMA)\n",
    "# print(\"Match:\", result)\n",
    "# if not result:\n",
    "#     print(\"Differences:\", diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_schemas(SCHEMA_MAPPING, NEW_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "from ragaai_catalyst import RagaAICatalyst\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=\"9Dr41iRXo4RhGLlLjnmq\",\n",
    "    secret_key=\"a7vEHZJrSdsZzkinTnXKZ3XNllzH0vu2EdooTNf5\",\n",
    "    base_url=\"https://llm-dev5.ragaai.ai/api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragaai_catalyst.prompt_manager import PromptManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manager = PromptManager(\"docker_auto_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hallu', 'Faithfulness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_manager.list_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list prompt versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v1': [{'content': '5y3hryht5yr', 'role': 'system'},\n",
       "  {'content': 'fbrbgrfbrfgbrgfbrfdb{{bfnbg}}', 'role': 'user'}],\n",
       " 'v2': [{'content': '5y3hryht5yr', 'role': 'system'},\n",
       "  {'content': 'fbrbgrfbrfgbrgfbrfdb{{bfnbg}}, {{dsss}}', 'role': 'user'}],\n",
       " 'v3': [{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct',\n",
       "   'role': 'system'},\n",
       "  {'content': 'This is the context: {{context}}\\n           This is LLM Response: {{llm_response}}\\n           This is user query: {{query}}\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text like:\\n           {{\"answer\": \"True\", \"reason\": \"reason for hallucination\",\"reason_type\": \"contradictory wrt context\"}}',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_manager.list_prompt_versions(\"hallu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v1': [{'content': \"You are an LLM evaluation tool specializing in faithfulness detection. Your task is to determine if an LLM's response is faithful to the provided context, considering the user's query.\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'Context (in FAQ format):\\n            {{context}}\\n\\n            User Query: {{query}}\\n\\n            LLM Response: {{llm_response}}\\n\\n            Evaluate the faithfulness of the LLM Response to the given Context and Query. Follow these guidelines:\\n            1. The response should only contain information present in the Context.\\n            2. The response should be relevant to the User Query.\\n            3. Any information in the response not found in the Context is considered unfaithful.\\n            4. Paraphrasing or rewording of Context information is acceptable if the meaning is preserved.\\n            5. The response should not contradict the Context.\\n            6. Omissions of relevant information from the Context are acceptable, but should be noted.\\n\\n            Verify 5 times before responding.\\n            Answer in True or False and give the reason why you believe this is the answer.\\n\\n            Return the answer in JSON format without any extra text like:\\n            {{\"answer\": \"True\", \"reason\": \"reason for faithfulness\"}}\\n\\n            Ensure your evaluation is precise and well-justified.',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_manager.list_prompt_versions(\"Faithfulness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get default prompt / specific version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ragaai_catalyst.prompt_manager.PromptObject at 0x138f25e40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_manager.get_prompt(\"hallu\", version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ragaai_catalyst.prompt_manager.PromptObject at 0x138f25570>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_manager.get_prompt(\"hallu\", version='v3')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'llm_response', 'query']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.get_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency_penalty': 0.0,\n",
       " 'max_tokens': 200,\n",
       " 'presence_penalty': 0.0,\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 1.0,\n",
       " 'model': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct',\n",
       "  'role': 'system'},\n",
       " {'content': 'This is the context: -----ghi-----\\n           This is LLM Response: -----def------\\n           This is user query: ---------\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text like:\\n           {{\"answer\": \"True\", \"reason\": \"reason for hallucination\",\"reason_type\": \"contradictory wrt context\"}}',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text=prompt.compile(query='---------',llm_response=\"-----def------\", context=\"-----ghi-----\")\n",
    "prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing variable(s): context",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-def-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/ragaai-catalyst/ragaai_catalyst/prompt_manager.py:329\u001b[0m, in \u001b[0;36mPromptObject.compile\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m extra_variables \u001b[38;5;241m=\u001b[39m provided_variables \u001b[38;5;241m-\u001b[39m required_variables\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_variables:\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing variable(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_variables)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_variables:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra variable(s) provided: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extra_variables)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing variable(s): context"
     ]
    }
   ],
   "source": [
    "prompt.compile(query='abc',llm_response=\"-def-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Extra variable(s) provided: abc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-def-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-ghi-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/ragaai-catalyst/ragaai_catalyst/prompt_manager.py:331\u001b[0m, in \u001b[0;36mPromptObject.compile\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing variable(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_variables)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_variables:\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra variable(s) provided: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extra_variables)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# compiled_prompt = self.text\u001b[39;00m\n\u001b[1;32m    335\u001b[0m user_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Extra variable(s) provided: abc"
     ]
    }
   ],
   "source": [
    "prompt.compile(query='abc',llm_response=\"-def-\", context=\"-ghi-\", abc=\"abc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value for variable 'query' must be a string, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-def-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-ghi-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/ragaai-catalyst/ragaai_catalyst/prompt_manager.py:339\u001b[0m, in \u001b[0;36mPromptObject.compile\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue for variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    340\u001b[0m     user_content \u001b[38;5;241m=\u001b[39m user_content\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, value)\n\u001b[1;32m    341\u001b[0m compiled_prompt \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_content \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext]\n",
      "\u001b[0;31mValueError\u001b[0m: Value for variable 'query' must be a string, not bool"
     ]
    }
   ],
   "source": [
    "prompt.compile(query=True,llm_response=\"-def-\", context=\"-ghi-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_string = \"This is a specific string replacing the original content\"\n",
    "\n",
    "data = [\n",
    "{\n",
    "    \"content\": \"You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct\",\n",
    "    \"role\": \"system\"\n",
    "},\n",
    "{\n",
    "    \"content\": \"This is the context: {{context}}\\n           This is LLM Response: {{llm_response}}\\n           This is user query: {{query}}\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text like:\\n           {{\\\"answer\\\": \\\"True\\\", \\\"reason\\\": \\\"reason for hallucination\\\",\\\"reason_type\\\": \\\"contradictory wrt context\\\"}}\",\n",
    "    \"role\": \"user\"\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = next(item[\"content\"] for item in data if item[\"role\"] == \"user\")\n",
    "print(user_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [{\"content\": specific_string if item[\"role\"] == \"user\" else item[\"content\"], \"role\": item[\"role\"]} for item in data]\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset error checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prod5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst import RagaAICatalyst\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=\"6sjaUDFNMdplU8boixSZ\",\n",
    "    secret_key=\"cT9K43hCUYWk0mZNlBTDpKJof2FobhBJuMJPXSeO\",\n",
    "    base_url=\"https://catalyst.raga.ai/api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The experiment name already exists in the project. Enter a unique experiment name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragaai_catalyst\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Experiment\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create an experiment\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m experiment_manager \u001b[38;5;241m=\u001b[39m \u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mECommerce-Logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError_UAT2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExperiment Description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mECommerce-Llama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add metrics to the experiment\u001b[39;00m\n\u001b[1;32m     12\u001b[0m experiment_manager\u001b[38;5;241m.\u001b[39madd_metrics(\n\u001b[1;32m     13\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHallucination\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[1;32m     15\u001b[0m     ]\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/package_error_check/ragaai-catalyst/ragaai_catalyst/experiment.py:66\u001b[0m, in \u001b[0;36mExperiment.__init__\u001b[0;34m(self, project_name, experiment_name, experiment_description, dataset_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# print(experiment_list)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_name \u001b[38;5;129;01min\u001b[39;00m experiment_list:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe experiment name already exists in the project. Enter a unique experiment name.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccess_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAGAAI_CATALYST_ACCESS_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecret_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAGAAI_CATALYST_SECRET_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The experiment name already exists in the project. Enter a unique experiment name."
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst import Experiment\n",
    "\n",
    "# Create an experiment\n",
    "experiment_manager = Experiment(\n",
    "    project_name=\"ECommerce-Logs\",\n",
    "    experiment_name=\"Error_UAT2\",\n",
    "    experiment_description=\"Experiment Description\",\n",
    "    dataset_name=\"ECommerce-Llama\",\n",
    ")\n",
    "\n",
    "# Add metrics to the experiment\n",
    "experiment_manager.add_metrics(\n",
    "    metrics=[\n",
    "      {\"name\": \"Hallucination\", \"config\": {\"model\": \"gpt-4o\", \"provider\":\"openai\"}}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get the status of the experiment\n",
    "status = experiment_manager.get_status()\n",
    "print(\"Experiment Status:\", status)\n",
    "\n",
    "# Get the results of the experiment\n",
    "results = experiment_manager.get_results()\n",
    "print(\"Experiment Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst import RagaAICatalyst\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=\"9Dr41iRXo4RhGLlLjnmq\",\n",
    "    secret_key=\"a7vEHZJrSdsZzkinTnXKZ3XNllzH0vu2EdooTNf5\",\n",
    "    base_url=\"https://llm-dev5.ragaai.ai/api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The experiment name already exists in the project. Enter a unique experiment name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragaai_catalyst\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Experiment\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create an experiment\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m experiment_manager \u001b[38;5;241m=\u001b[39m \u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocker_auto_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgt_data_v4_gpt-4o-mini_ValidPython_v_2_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExperiment Description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgt_data_v5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add metrics to the experiment\u001b[39;00m\n\u001b[1;32m     12\u001b[0m experiment_manager\u001b[38;5;241m.\u001b[39madd_metrics(\n\u001b[1;32m     13\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHallucination\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[1;32m     15\u001b[0m     ]\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/catalyst_error_handling/package_error_check/ragaai-catalyst/ragaai_catalyst/experiment.py:66\u001b[0m, in \u001b[0;36mExperiment.__init__\u001b[0;34m(self, project_name, experiment_name, experiment_description, dataset_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# print(experiment_list)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_name \u001b[38;5;129;01min\u001b[39;00m experiment_list:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe experiment name already exists in the project. Enter a unique experiment name.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccess_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAGAAI_CATALYST_ACCESS_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecret_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAGAAI_CATALYST_SECRET_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The experiment name already exists in the project. Enter a unique experiment name."
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst import Experiment\n",
    "\n",
    "# Create an experiment\n",
    "experiment_manager = Experiment(\n",
    "    project_name=\"docker_auto_test\",\n",
    "    experiment_name=\"gt_data_v4_gpt-4o-mini_ValidPython_v_2_1\",\n",
    "    experiment_description=\"Experiment Description\",\n",
    "    dataset_name=\"gt_data_v5\",\n",
    ")\n",
    "\n",
    "# Add metrics to the experiment\n",
    "experiment_manager.add_metrics(\n",
    "    metrics=[\n",
    "      {\"name\": \"Hallucination\", \"config\": {\"model\": \"gpt-4o\", \"provider\":\"openai\"}}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get the status of the experiment\n",
    "status = experiment_manager.get_status()\n",
    "print(\"Experiment Status:\", status)\n",
    "\n",
    "# Get the results of the experiment\n",
    "results = experiment_manager.get_results()\n",
    "print(\"Experiment Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raga_ai_catalyst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
