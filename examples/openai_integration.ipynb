{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ragaai_catalyst.prompt_manager import PromptManager\n",
    "from ragaai_catalyst import RagaAICatalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "catalyst = RagaAICatalyst(\n",
    "    access_key=\"9Dr41iRXo4RhGLlLjnmq\",\n",
    "    secret_key=\"a7vEHZJrSdsZzkinTnXKZ3XNllzH0vu2EdooTNf5\",\n",
    "    base_url=\"https://llm-dev5.ragaai.ai/api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PromptManager\n",
    "prompt_manager = PromptManager(\"docker_auto_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-hallucination example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt\n",
    "prompt = prompt_manager.get_prompt(\"hallu\", version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_variables:  ['context', 'llm_response', 'query']\n"
     ]
    }
   ],
   "source": [
    "# Get prompt variables\n",
    "prompt_variables = prompt.get_variables()\n",
    "print(\"prompt_variables: \",prompt_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  {'frequency_penalty': 0.0, 'max_tokens': 200, 'presence_penalty': 0.0, 'temperature': 0.5, 'top_p': 1.0, 'model': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "# Get parameters\n",
    "parameters = prompt.get_parameters()\n",
    "print(\"parameters: \",parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct', 'role': 'system'}, {'content': 'This is the context: France is a country in Western Europe. Its capital and largest city is Paris.\\n           This is LLM Response: The capital of France is Paris.\\n           This is user query: What is the capital of France?\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text like:\\n           {{\"answer\": \"True\", \"reason\": \"reason for hallucination\",\"reason_type\": \"contradictory wrt context\"}}', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "# Compile the prompt\n",
    "compiled_prompt = prompt.compile(\n",
    "    query='What is the capital of France?',\n",
    "    llm_response=\"The capital of France is Paris.\",\n",
    "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\"\n",
    ")\n",
    "print(compiled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call OpenAI API\n",
    "def get_openai_response(prompt):\n",
    "    client = openai.OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=parameters[\"model\"],\n",
    "        messages=prompt\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response:\n",
      "{\"answer\": \"False\"}\n"
     ]
    }
   ],
   "source": [
    "# Get response from OpenAI\n",
    "openai_response = get_openai_response(compiled_prompt)\n",
    "\n",
    "print(\"OpenAI Response:\")\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct', 'role': 'system'}, {'content': 'This is the context: France is a country in Western Europe. Its capital and largest city is Paris.\\n           This is LLM Response: The capital of France is Lyon.\\n           This is user query: What is the capital of France?\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text like:\\n           {{\"answer\": \"True\", \"reason\": \"reason for hallucination\",\"reason_type\": \"contradictory wrt context\"}}', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "# Compile the prompt\n",
    "compiled_prompt = prompt.compile(\n",
    "    query='What is the capital of France?',\n",
    "    llm_response=\"The capital of France is Lyon.\",\n",
    "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\"\n",
    ")\n",
    "print(compiled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response:\n",
      "{ \"answer\": \"True\", \"reason\": \"The response is hallucinating because Lyon is mentioned as the capital of France, which contradicts the fact presented in the context that Paris is the capital of France.\", \"reason_type\": \"contradictory wrt context\" }\n"
     ]
    }
   ],
   "source": [
    "# Get response from OpenAI\n",
    "openai_response = get_openai_response(compiled_prompt)\n",
    "\n",
    "print(\"OpenAI Response:\")\n",
    "print(openai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raga_ai_catalyst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
