{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install litellm -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharthakosti/anaconda3/envs/raga_ai_catalyst/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'underscore_attrs_are_private' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ragaai_catalyst.prompt_manager import PromptManager\n",
    "from ragaai_catalyst import RagaAICatalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "catalyst = RagaAICatalyst(\n",
    "    access_key=\"9Dr41iRXo4RhGLlLjnmq\",\n",
    "    secret_key=\"a7vEHZJrSdsZzkinTnXKZ3XNllzH0vu2EdooTNf5\",\n",
    "    base_url=\"https://llm-dev5.ragaai.ai/api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_manager = PromptManager(\"docker_auto_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-hallucination example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt\n",
    "prompt = prompt_manager.get_prompt(\"hallu\", version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_variables:  ['context', 'llm_response', 'query']\n"
     ]
    }
   ],
   "source": [
    "# Get prompt variables\n",
    "prompt_variables = prompt.get_variables()\n",
    "print(\"prompt_variables: \",prompt_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  {'frequency_penalty': 0.0, 'max_tokens': 200, 'presence_penalty': 0.0, 'temperature': 0.5, 'top_p': 1.0, 'model': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "# Get parameters\n",
    "parameters = prompt.get_parameters()\n",
    "print(\"parameters: \",parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct', 'role': 'system'}, {'content': 'This is the context: France is a country in Western Europe. Its capital and largest city is Paris.\\n           This is LLM Response: The capital of France is Paris.\\n           This is user query: What is the capital of France?\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text like:\\n           {{\"answer\": \"True\", \"reason\": \"reason for hallucination\",\"reason_type\": \"contradictory wrt context\"}}', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "# Compile the prompt\n",
    "compiled_prompt = prompt.compile(\n",
    "    query='What is the capital of France?',\n",
    "    llm_response=\"The capital of France is Paris.\",\n",
    "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\"\n",
    ")\n",
    "print(compiled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call LiteLLM API\n",
    "def get_litellm_response(prompt):\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=prompt\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:31:38 - LiteLLM:INFO\u001b[0m: utils.py:2977 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m08:31:39 - LiteLLM:INFO\u001b[0m: utils.py:1022 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiteLLM Response:\n",
      "{\"answer\": \"False\", \"reason\": \"The LLM Response accurately reflects the information present in the Context, which states that the capital of France is Paris. There are no new claims or incorrect information being presented.\",\"reason_type\": \"correct response wrt query\"}\n"
     ]
    }
   ],
   "source": [
    "# Get response from LiteLLM\n",
    "litellm_response = get_litellm_response(compiled_prompt)\n",
    "\n",
    "print(\"LiteLLM Response:\")\n",
    "print(litellm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct', 'role': 'system'}, {'content': 'This is the context: France is a country in Western Europe. Its capital and largest city is Paris.\\n           This is LLM Response: The capital of France is Lyon.\\n           This is user query: What is the capital of France?\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text like:\\n           {{\"answer\": \"True\", \"reason\": \"reason for hallucination\",\"reason_type\": \"contradictory wrt context\"}}', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "# Compile the prompt\n",
    "compiled_prompt = prompt.compile(\n",
    "    query='What is the capital of France?',\n",
    "    llm_response=\"The capital of France is Lyon.\",\n",
    "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\"\n",
    ")\n",
    "print(compiled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:32:19 - LiteLLM:INFO\u001b[0m: utils.py:2977 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m08:32:20 - LiteLLM:INFO\u001b[0m: utils.py:1022 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiteLLM Response:\n",
      "{\"answer\": \"True\", \"reason\": \"The response incorrectly states that the capital of France is Lyon, while the context explicitly states that the capital is Paris.\",\"reason_type\": \"Contradictory wrt context\"}\n"
     ]
    }
   ],
   "source": [
    "# Get response from LiteLLM\n",
    "litellm_response = get_litellm_response(compiled_prompt)\n",
    "\n",
    "print(\"LiteLLM Response:\")\n",
    "print(litellm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raga_ai_catalyst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
